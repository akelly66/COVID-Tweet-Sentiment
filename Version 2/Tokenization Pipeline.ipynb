{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coronavirus Tweets: Pandemic Panic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T01:46:46.704248Z",
     "start_time": "2020-10-14T01:46:46.695811Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import re \n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from tweet_processing_funcs import *\n",
    "from nltk import FreqDist\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from lime import lime_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T01:45:25.100342Z",
     "start_time": "2020-10-14T01:45:23.456374Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>extended_tweet</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>filter_level</th>\n",
       "      <th>...</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "      <th>expanded</th>\n",
       "      <th>full_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri Jan 31 23:58:59 +0000 2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'urls': [{'url': 'https://t.c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'created_at': 'Fri Jan 31 23:30:08 +0000 2020...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>RT @CNN: There have been more than 9,800 cases...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 292918761, 'id_str': '292918761', 'name...</td>\n",
       "      <td>{'created_at': 'Fri Jan 31 23:30:08 +0000 2020...</td>\n",
       "      <td>RT @CNN: There have been more than 9,800 cases...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri Jan 31 23:58:59 +0000 2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'urls': [], 'user_mentions': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'created_at': 'Fri Jan 31 22:19:00 +0000 2020...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>RT @JaneLytv: 43. Zero Hedge, a pro-Trump webs...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 262334207, 'id_str': '262334207', 'name...</td>\n",
       "      <td>{'created_at': 'Fri Jan 31 22:19:00 +0000 2020...</td>\n",
       "      <td>43. Zero Hedge, a pro-Trump website, has doxxe...</td>\n",
       "      <td>{'neg': 0.096, 'neu': 0.853, 'pos': 0.05, 'com...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri Jan 31 23:58:59 +0000 2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'urls': [], 'user_mentions': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'created_at': 'Fri Jan 31 21:08:39 +0000 2020...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>RT @skarlamangla: This flu season has killed 1...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 2451840349, 'id_str': '2451840349', 'na...</td>\n",
       "      <td>{'created_at': 'Fri Jan 31 21:08:39 +0000 2020...</td>\n",
       "      <td>RT @skarlamangla: This flu season has killed 1...</td>\n",
       "      <td>{'neg': 0.429, 'neu': 0.571, 'pos': 0.0, 'comp...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri Jan 31 23:58:59 +0000 2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'urls': [], 'user_mentions': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'created_at': 'Fri Jan 31 05:06:49 +0000 2020...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>RT @jamesmassola: Medical experts raise concer...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 513208128, 'id_str': '513208128', 'name...</td>\n",
       "      <td>{'created_at': 'Fri Jan 31 05:06:49 +0000 2020...</td>\n",
       "      <td>Medical experts raise concerns about Indonesia...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.846, 'pos': 0.154, 'comp...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri Jan 31 23:58:59 +0000 2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'urls': [], 'user_mentions': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'created_at': 'Fri Jan 31 01:27:50 +0000 2020...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>RT @popplioikawa: the flu: *results in 500,000...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 993138436667314176, 'id_str': '99313843...</td>\n",
       "      <td>{'created_at': 'Fri Jan 31 01:27:50 +0000 2020...</td>\n",
       "      <td>the flu: *results in 500,000 hospitalizations ...</td>\n",
       "      <td>{'neg': 0.113, 'neu': 0.845, 'pos': 0.042, 'co...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   contributors  coordinates                      created_at  \\\n",
       "0           NaN          NaN  Fri Jan 31 23:58:59 +0000 2020   \n",
       "1           NaN          NaN  Fri Jan 31 23:58:59 +0000 2020   \n",
       "2           NaN          NaN  Fri Jan 31 23:58:59 +0000 2020   \n",
       "3           NaN          NaN  Fri Jan 31 23:58:59 +0000 2020   \n",
       "4           NaN          NaN  Fri Jan 31 23:58:59 +0000 2020   \n",
       "\n",
       "  display_text_range                                           entities  \\\n",
       "0                NaN  {'hashtags': [], 'urls': [{'url': 'https://t.c...   \n",
       "1                NaN  {'hashtags': [], 'urls': [], 'user_mentions': ...   \n",
       "2                NaN  {'hashtags': [], 'urls': [], 'user_mentions': ...   \n",
       "3                NaN  {'hashtags': [], 'urls': [], 'user_mentions': ...   \n",
       "4                NaN  {'hashtags': [], 'urls': [], 'user_mentions': ...   \n",
       "\n",
       "  extended_entities extended_tweet  favorite_count  favorited filter_level  \\\n",
       "0               NaN            NaN               0      False          low   \n",
       "1               NaN            NaN               0      False          low   \n",
       "2               NaN            NaN               0      False          low   \n",
       "3               NaN            NaN               0      False          low   \n",
       "4               NaN            NaN               0      False          low   \n",
       "\n",
       "   ...  retweeted                                   retweeted_status  \\\n",
       "0  ...      False  {'created_at': 'Fri Jan 31 23:30:08 +0000 2020...   \n",
       "1  ...      False  {'created_at': 'Fri Jan 31 22:19:00 +0000 2020...   \n",
       "2  ...      False  {'created_at': 'Fri Jan 31 21:08:39 +0000 2020...   \n",
       "3  ...      False  {'created_at': 'Fri Jan 31 05:06:49 +0000 2020...   \n",
       "4  ...      False  {'created_at': 'Fri Jan 31 01:27:50 +0000 2020...   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "4  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "\n",
       "                                                text  truncated  \\\n",
       "0  RT @CNN: There have been more than 9,800 cases...      False   \n",
       "1  RT @JaneLytv: 43. Zero Hedge, a pro-Trump webs...      False   \n",
       "2  RT @skarlamangla: This flu season has killed 1...      False   \n",
       "3  RT @jamesmassola: Medical experts raise concer...      False   \n",
       "4  RT @popplioikawa: the flu: *results in 500,000...      False   \n",
       "\n",
       "                                                user  \\\n",
       "0  {'id': 292918761, 'id_str': '292918761', 'name...   \n",
       "1  {'id': 262334207, 'id_str': '262334207', 'name...   \n",
       "2  {'id': 2451840349, 'id_str': '2451840349', 'na...   \n",
       "3  {'id': 513208128, 'id_str': '513208128', 'name...   \n",
       "4  {'id': 993138436667314176, 'id_str': '99313843...   \n",
       "\n",
       "                                            expanded  \\\n",
       "0  {'created_at': 'Fri Jan 31 23:30:08 +0000 2020...   \n",
       "1  {'created_at': 'Fri Jan 31 22:19:00 +0000 2020...   \n",
       "2  {'created_at': 'Fri Jan 31 21:08:39 +0000 2020...   \n",
       "3  {'created_at': 'Fri Jan 31 05:06:49 +0000 2020...   \n",
       "4  {'created_at': 'Fri Jan 31 01:27:50 +0000 2020...   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  RT @CNN: There have been more than 9,800 cases...   \n",
       "1  43. Zero Hedge, a pro-Trump website, has doxxe...   \n",
       "2  RT @skarlamangla: This flu season has killed 1...   \n",
       "3  Medical experts raise concerns about Indonesia...   \n",
       "4  the flu: *results in 500,000 hospitalizations ...   \n",
       "\n",
       "                                           sentiment polarity  \n",
       "0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...      0.5  \n",
       "1  {'neg': 0.096, 'neu': 0.853, 'pos': 0.05, 'com...      0.0  \n",
       "2  {'neg': 0.429, 'neu': 0.571, 'pos': 0.0, 'comp...      0.0  \n",
       "3  {'neg': 0.0, 'neu': 0.846, 'pos': 0.154, 'comp...      1.0  \n",
       "4  {'neg': 0.113, 'neu': 0.845, 'pos': 0.042, 'co...      0.0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweet_polarity.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T01:50:28.084836Z",
     "start_time": "2020-10-14T01:50:28.068283Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import contractions\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "def remove_url_and_RT(row):\n",
    "    '''\n",
    "    This function takes each tweet\n",
    "    and removes the urls and retweet\n",
    "    indicator from them.\n",
    "    '''\n",
    "    row = re.sub('https://[A-Za-z0-9./]+',\"\",row)\n",
    "    row = re.sub('http://[A-Za-z0-9./]+',\"\",row)\n",
    "    row = re.sub('^RT',\"\", row)\n",
    "    return row\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "\n",
    "    '''\n",
    "    This function takes a tweet variable,\n",
    "    removes punctuation and linebreaks,\n",
    "    sets all words to lowercase, and\n",
    "    returns the cleaned tweet as a single\n",
    "    variable list.\n",
    "    '''\n",
    "\n",
    "    # Grabbing most common punctuation symbols and ellipsis symbol\n",
    "    punctuation_list = list(string.punctuation)+ [\"…\"] + ['’']\n",
    "    punctuation_list.remove('#')\n",
    "\n",
    "\n",
    "    cleaned_tweet = []\n",
    "\n",
    "    for symbol in punctuation_list:\n",
    "\n",
    "        tweet = tweet.replace(symbol, \"\").lower()\n",
    "\n",
    "        # Removing trailing characters\n",
    "        tweet = tweet.rstrip()\n",
    "\n",
    "        # Cleaning non-ASCII characters\n",
    "        tweet = re.sub(\"([^\\x00-\\x7F])+\",\"\",tweet)\n",
    "\n",
    "    cleaned_tweet.append(tweet)\n",
    "\n",
    "    return cleaned_tweet\n",
    "\n",
    "def tokenize(clean_tweet):\n",
    "\n",
    "    '''\n",
    "    This function takes a cleaned tweet,\n",
    "    joins into one string (if not already),\n",
    "    runs the tweet through NLTK work tokenizer,\n",
    "    removes English stopwords, replaces \"us\"\n",
    "    with \"usa,\" removes numbers and returns\n",
    "    the tokenized tweet in list format.\n",
    "    '''\n",
    "\n",
    "    joined_tweet = ' '.join(clean_tweet)\n",
    "    stopwords_list = stopwords.words('english')\n",
    "\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokenized_tweet = tokenizer.tokenize(joined_tweet)\n",
    "    # Removing stopwords\n",
    "    tokenized_tweet = [word for word in tokenized_tweet if word not in stopwords_list]\n",
    "\n",
    "    # Subbing 'usa' for 'us'\n",
    "    tokenized_tweet = ['usa' if word == 'us' else word for word in tokenized_tweet]\n",
    "\n",
    "    # Removing numbers\n",
    "    tokenized_tweet = [word for word in tokenized_tweet if not word.isnumeric()]\n",
    "\n",
    "    return tokenized_tweet\n",
    "\n",
    "def lem_tweet(tweet):\n",
    "    '''\n",
    "    This function takes a tweet in\n",
    "    the form of a tokenized\n",
    "    word list and lemmatizes it.\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lemmed_tweet = [lemmatizer.lemmatize(word) for word in tweet]\n",
    "\n",
    "    return lemmed_tweet\n",
    "\n",
    "def stem_tweet(tweet):\n",
    "\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_tweet = [stemmer.stem(word) for word in tweet]\n",
    "\n",
    "    return stemmed_tweet\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    This function takes an original\n",
    "    tweet, cleans, tokenizes,\n",
    "    and lemmatizes the tweet.\n",
    "    '''\n",
    "    sans_url_tweet = remove_url_and_RT(tweet)\n",
    "    cleaned = clean_tweet(sans_url_tweet)\n",
    "    tokenized = tokenize(cleaned)\n",
    "    stemmed_tweet = stem_tweet(tokenized)\n",
    "    lemmed_tweet = lem_tweet(stemmed_tweet)\n",
    "\n",
    "    return lemmed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T01:54:49.258133Z",
     "start_time": "2020-10-14T01:54:32.506311Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets = df['full_text']\n",
    "processed_tweets = tweets.apply(process_tweet)\n",
    "processed_tweets = processed_tweets.apply((' ').join)\n",
    "labels = df['polarity'].astype('str')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_tweets, labels, test_size=.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T23:49:16.424755Z",
     "start_time": "2020-10-13T23:49:16.396612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    4346\n",
       "1.0    2774\n",
       "0.5    2180\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T23:49:58.122895Z",
     "start_time": "2020-10-13T23:49:58.094766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    3908\n",
       "1.0    2497\n",
       "0.5    1965\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T01:53:52.068413Z",
     "start_time": "2020-10-14T01:53:52.063097Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizer(tweet):\n",
    "    tknzr = TweetTokenizer(strip_handles=True, preserve_case=True)\n",
    "    return tknzr.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf Vectorization\n",
    "\n",
    "Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T01:49:00.142998Z",
     "start_time": "2020-10-14T01:49:00.128170Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_unigram = TfidfVectorizer(stop_words='english', tokenizer=tokenizer, preprocessor=None)\n",
    "tfidf_bigram = TfidfVectorizer(stop_words='english', tokenizer=tokenizer, ngram_range=(1,2))\n",
    "tfidf_trigram = TfidfVectorizer(stop_words='english', tokenizer=tokenizer, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorization\n",
    "\n",
    "Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T23:41:10.171184Z",
     "start_time": "2020-10-13T23:41:10.167174Z"
    }
   },
   "outputs": [],
   "source": [
    "count_unigram = CountVectorizer(stop_words='english', tokenizer=tokenizer)\n",
    "count_bigram = CountVectorizer(stop_words='english', tokenizer=tokenizer, ngram_range=(1,2))\n",
    "count_trigram = CountVectorizer(stop_words='english', tokenizer=tokenizer, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLoVe\n",
    "\n",
    "Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithms and Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T00:51:07.515927Z",
     "start_time": "2020-10-14T00:51:07.509853Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T01:16:07.029943Z",
     "start_time": "2020-10-14T01:16:07.023981Z"
    }
   },
   "outputs": [],
   "source": [
    "def classification(X_train, y_train):\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression()\n",
    "    lr_model = lr.fit(X_train, y_train)\n",
    "    # Multinomail Naive Bayes\n",
    "    nb = MultinomialNB()\n",
    "    nb_model = nb.fit(X_train, y_train)\n",
    "    # Random Forest\n",
    "    rf = RandomForest()\n",
    "    rf_model = rf.fit(X_train, y_train)\n",
    "    # Decision Trees\n",
    "    dt = DecisionTreeClassifier(criterion='entropy')\n",
    "    dt_model = dt.fit(X_train, y_train)\n",
    "    # XGBoost\n",
    "    xgb = xgb.XGBClassifier()\n",
    "    xgb_model = xgb.fit(X_train, y_train)\n",
    "    # Support Vector Machines\n",
    "    svm = SVC(kernel='linear')\n",
    "    svm_model = svm.fit(X_train, y_train)\n",
    "    \n",
    "    return [lr_model, nb_model, rf_model, dt_model, xgb_model, svm_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T02:00:06.417001Z",
     "start_time": "2020-10-14T02:00:06.411726Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_validation(vectorizer, X_train, X_test, y_test):\n",
    "    accuracy_scores = {}\n",
    "    \n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    \n",
    "    classifiers = classification(X_train, X_test)\n",
    "    for classifier in classifiers:\n",
    "        preds = classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, preds)\n",
    "        accuracy_scores[classifier] = accuracy\n",
    "        \n",
    "        \n",
    "        \n",
    "    accuracy_df = pd.from_dict(accuracy_scores, columns = ['Model', 'Accuracy'])\n",
    "    return accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-default-env] *",
   "language": "python",
   "name": "conda-env-.conda-default-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
