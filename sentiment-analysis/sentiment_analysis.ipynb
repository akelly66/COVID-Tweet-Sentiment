{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of COVID-19 Tweets: When did the Public Panic Set In? Part 4: Supervised Classification Modeling\n",
    "\n",
    "    Notebook by Allison Kelly - allisonkelly42@gmail.com\n",
    "    \n",
    "This notebook is preceded by parts <a href=\"https://github.com/akelly66/COVID-Tweet-Sentiment/blob/master/tweet-scraping/Twitter-API-Scraping.ipynb\">1</a>, <a href=\"https://github.com/akelly66/COVID-Tweet-Sentiment/blob/master/text-processing/NLP-Text-Processing.ipynb\">2</a> and <a href=\"https://github.com/akelly66/COVID-Tweet-Sentiment/blob/master/EDA/tweet-EDA.ipynb\">3</a>. Part 4 will focus on the modeling portion, but is still very much in ins infancy. Markdown cells and complete documentation to come. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:18:51.420196Z",
     "start_time": "2020-09-29T18:18:46.219237Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from nltk import word_tokenize\n",
    "from ast import literal_eval\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:19:50.975141Z",
     "start_time": "2020-09-29T18:18:54.156463Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>processed_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[switchfoot, httptwitpiccom, 2y1zl, awww, that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[kenichan, dived, many, time, ball, managed, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[nationwideclass, behaving, im, mad, cant, see]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                                   processed_tweets\n",
       "0         0  [switchfoot, httptwitpiccom, 2y1zl, awww, that...\n",
       "1         0  [upset, cant, update, facebook, texting, might...\n",
       "2         0  [kenichan, dived, many, time, ball, managed, s...\n",
       "3         0             [whole, body, feel, itchy, like, fire]\n",
       "4         0    [nationwideclass, behaving, im, mad, cant, see]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets = pd.read_csv('Data/processed_train.csv', \n",
    "                       usecols=['polarity', 'processed_tweets'],\n",
    "                       # Converting string to list\n",
    "                       converters={\"processed_tweets\": literal_eval})\n",
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:19:50.988234Z",
     "start_time": "2020-09-29T18:19:50.979275Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:19:51.314665Z",
     "start_time": "2020-09-29T18:19:51.008333Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    12529\n",
       "0    12471\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample = train_tweets.sample(n=25000, random_state = 42)\n",
    "train_sample.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:19:51.347554Z",
     "start_time": "2020-09-29T18:19:51.321067Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_sample['processed_tweets'], \n",
    "                                                    train_sample['polarity'], \n",
    "                                                    test_size=.20, \n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:19:51.423386Z",
     "start_time": "2020-09-29T18:19:51.350715Z"
    }
   },
   "outputs": [],
   "source": [
    "all_words_list = [item for sublist in X_train for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:19:51.485857Z",
     "start_time": "2020-09-29T18:19:51.431376Z"
    }
   },
   "outputs": [],
   "source": [
    "total_vocab = set(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:19:51.506386Z",
     "start_time": "2020-09-29T18:19:51.492986Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31349"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(all_words_list))\n",
    "len(total_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T21:09:33.086233Z",
     "start_time": "2020-09-29T21:09:33.080689Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T21:09:33.452864Z",
     "start_time": "2020-09-29T21:09:33.414168Z"
    }
   },
   "outputs": [],
   "source": [
    "train_tweet_list = X_train.apply(('').join)\n",
    "test_tweet_list = X_test.apply(('').join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T21:09:33.955858Z",
     "start_time": "2020-09-29T21:09:33.711078Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_train = vectorizer.fit_transform(train_tweet_list)\n",
    "tfidf_test = vectorizer.transform(test_tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T21:08:07.149083Z",
     "start_time": "2020-09-29T21:08:07.140663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 20269)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:19:51.990096Z",
     "start_time": "2020-09-29T18:19:51.979499Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "# rf_classifier = RandomForestClassifier(n_estimators=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:19:52.032971Z",
     "start_time": "2020-09-29T18:19:52.000448Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "nb_train_preds = nb_classifier.predict(tfidf_train)\n",
    "nb_test_preds = nb_classifier.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:19:52.059990Z",
     "start_time": "2020-09-29T18:19:52.038983Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes\n",
      "Training Accuracy: 0.9993 \t\t Testing Accuracy: 0.4968\n"
     ]
    }
   ],
   "source": [
    "nb_train_score = accuracy_score(y_train, nb_train_preds)\n",
    "nb_test_score = accuracy_score(y_test, nb_test_preds)\n",
    "print(\"Multinomial Naive Bayes\")\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(nb_train_score, nb_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T21:41:27.834817Z",
     "start_time": "2020-09-29T21:41:20.559434Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with sag Solver and C=0.001\n",
      "Training Accuracy: 0.5038 \t\t Testing Accuracy: 0.4968\n",
      "\n",
      "Logistic Regression with sag Solver and C=0.01\n",
      "Training Accuracy: 0.9988 \t\t Testing Accuracy: 0.4968\n",
      "\n",
      "Logistic Regression with sag Solver and C=0.1\n",
      "Training Accuracy: 0.9989 \t\t Testing Accuracy: 0.4968\n",
      "\n",
      "Logistic Regression with sag Solver and C=1\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n",
      "Logistic Regression with sag Solver and C=10\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Allie/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with sag Solver and C=100\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Allie/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with sag Solver and C=1000\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n",
      "Logistic Regression with saga Solver and C=0.001\n",
      "Training Accuracy: 0.5036 \t\t Testing Accuracy: 0.4964\n",
      "\n",
      "Logistic Regression with saga Solver and C=0.01\n",
      "Training Accuracy: 0.9988 \t\t Testing Accuracy: 0.4968\n",
      "\n",
      "Logistic Regression with saga Solver and C=0.1\n",
      "Training Accuracy: 0.9989 \t\t Testing Accuracy: 0.4968\n",
      "\n",
      "Logistic Regression with saga Solver and C=1\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n",
      "Logistic Regression with saga Solver and C=10\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Allie/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with saga Solver and C=100\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Allie/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with saga Solver and C=1000\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n",
      "Logistic Regression with lbfgs Solver and C=0.001\n",
      "Training Accuracy: 0.5038 \t\t Testing Accuracy: 0.4968\n",
      "\n",
      "Logistic Regression with lbfgs Solver and C=0.01\n",
      "Training Accuracy: 0.9988 \t\t Testing Accuracy: 0.4968\n",
      "\n",
      "Logistic Regression with lbfgs Solver and C=0.1\n",
      "Training Accuracy: 0.9989 \t\t Testing Accuracy: 0.4968\n",
      "\n",
      "Logistic Regression with lbfgs Solver and C=1\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n",
      "Logistic Regression with lbfgs Solver and C=10\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Allie/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with lbfgs Solver and C=100\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n",
      "Logistic Regression with lbfgs Solver and C=1000\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Allie/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "solvers = ['sag','saga','lbfgs']\n",
    "C = [.001, .01, .1, 1, 10, 100,1000]\n",
    "\n",
    "for solver in solvers:\n",
    "    for c in C:\n",
    "        lr_classifier = LogisticRegression(verbose=0, solver=solver,C=c, random_state=0)\n",
    "        lr_model = lr_classifier.fit(tfidf_train, y_train)\n",
    "    \n",
    "        lr_train_score = lr_model.score(tfidf_train, y_train)\n",
    "        lr_test_score = lr_model.score(tfidf_test, y_test)\n",
    "    \n",
    "        print(f\"Logistic Regression with {solver} Solver and C={c}\")\n",
    "        print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\\n\".format(lr_train_score, lr_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T23:10:09.488394Z",
     "start_time": "2020-09-24T23:09:55.866205Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T23:11:37.235055Z",
     "start_time": "2020-09-24T23:10:09.493008Z"
    }
   },
   "outputs": [],
   "source": [
    "dense_matrix = tfidf_train.todense()\n",
    "dense_list = dense_matrix.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:20:16.337098Z",
     "start_time": "2020-09-29T18:20:16.329554Z"
    }
   },
   "outputs": [],
   "source": [
    "# AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# tfidf_train = tfidf_train.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# tfidf_test = tfidf_test.cache().prefetch(buffer_size=AUTOTUNE)b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
