{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of COVID-19 Tweets: When did the Public Panic Set In? Part 4: Supervised Classification Modeling\n",
    "\n",
    "    Notebook by Allison Kelly - allisonkelly42@gmail.com\n",
    "    \n",
    "This notebook is preceded by parts <a href=\"https://github.com/akelly66/COVID-Tweet-Sentiment/blob/master/tweet-scraping/Twitter-API-Scraping.ipynb\">1</a>, <a href=\"https://github.com/akelly66/COVID-Tweet-Sentiment/blob/master/text-processing/NLP-Text-Processing.ipynb\">2</a> and <a href=\"https://github.com/akelly66/COVID-Tweet-Sentiment/blob/master/EDA/tweet-EDA.ipynb\">3</a>. Part 4 will focus on the modeling portion, but is still very much in ins infancy. Markdown cells and complete documentation to come. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:03:21.001848Z",
     "start_time": "2020-10-07T23:03:20.997094Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "from ast import literal_eval\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data was derived by Sentiment 140, a class project from Stanford University. It is a set of 1,600,000 tweets that are categorized by polarity, where 0 is negative, 2 is neutral, and 4 is positive. The polarity of the tweets was determined by the use of emojis where 'happy' emojis were positive and 'unhappy' emojis were negative. You can find more information and data download <a href=\"http://help.sentiment140.com/for-students\">here.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T20:24:17.687898Z",
     "start_time": "2020-10-07T20:23:09.887844Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>processed_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[switchfoot, httptwitpiccom, 2y1zl, awww, that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[kenichan, dived, many, time, ball, managed, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[nationwideclass, behaving, im, mad, cant, see]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                                   processed_tweets\n",
       "0         0  [switchfoot, httptwitpiccom, 2y1zl, awww, that...\n",
       "1         0  [upset, cant, update, facebook, texting, might...\n",
       "2         0  [kenichan, dived, many, time, ball, managed, s...\n",
       "3         0             [whole, body, feel, itchy, like, fire]\n",
       "4         0    [nationwideclass, behaving, im, mad, cant, see]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets = pd.read_csv('Data/processed_train.csv', \n",
    "                       usecols=['polarity', 'processed_tweets'],\n",
    "                       # Converting string to list\n",
    "                       converters={\"processed_tweets\": literal_eval})\n",
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T22:45:57.576293Z",
     "start_time": "2020-10-07T22:45:57.555019Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in dataset: 1600000\n",
      "\n",
      "Distribution of target classes: \n",
      "4    800000\n",
      "0    800000\n",
      "Name: polarity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of tweets in dataset: {len(train_tweets)}\\n\")\n",
    "print(f\"Distribution of target classes: \\n{train_tweets.polarity.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of more than 1.5 million tweets, and evenly distributed between negative and positive sentiments, however there are none in the neutral class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll split it into training and test sets. Because of the size of the dataset, it will be too computationally expensive to model so we'll take a random sample of 25,000 tweets before we do the split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T20:24:17.698356Z",
     "start_time": "2020-10-07T20:24:17.692149Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T20:24:17.901970Z",
     "start_time": "2020-10-07T20:24:17.709597Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    12529\n",
       "0    12471\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample = train_tweets.sample(n=25000, random_state = 42)\n",
    "train_sample.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T22:49:37.388855Z",
     "start_time": "2020-10-07T22:49:37.382893Z"
    }
   },
   "source": [
    "The distribution of classes is nearly equal, which is great and will make for a more accurate classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T20:24:17.928583Z",
     "start_time": "2020-10-07T20:24:17.908621Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_sample['processed_tweets'], \n",
    "                                                    train_sample['polarity'], \n",
    "                                                    test_size=.20, \n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T20:24:18.012168Z",
     "start_time": "2020-10-07T20:24:17.932447Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting number of words in all tweets combined\n",
    "all_words_list = [item for sublist in X_train for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T20:24:18.067343Z",
     "start_time": "2020-10-07T20:24:18.032440Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting unique words\n",
    "total_vocab = set(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:02:19.407739Z",
     "start_time": "2020-10-07T23:02:19.402479Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in all tweets: 154984\n",
      "Total unique words: 31349\n",
      "Percentage of words that are unique: 20.23\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of words in all tweets: {len(all_words_list)}\")\n",
    "print(f\"Total unique words: {len(total_vocab)}\")\n",
    "print(f\"Percentage of words that are unique: {round(len(total_vocab)/len(all_words_list),4)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Word Embeddings with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got our tokenized tweets split into training and test sets, but they're still just a bunch of words. We'll need to vectorize them by training a neural network, resulting in a high-dimensional embedding space with each word as a unique vector. We can then use those vectors and their proximity to others to identify semantic relationships between words.\n",
    "\n",
    "The Word2Vec model we'll initiate takes five parameters (from documentation):\n",
    "\n",
    "* The training data (called 'sentences' in the documentation)\n",
    "* size : Dimensionality of the word vectors.\n",
    "* window : Maximum distance between the current and predicted word within a sentence.\n",
    "* min_count : Ignores all words with total frequency lower than this.\n",
    "* workers : Use these many worker threads to train the model (=faster training with multicore machines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:03:55.109718Z",
     "start_time": "2020-10-07T23:03:39.909430Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(X_train, size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:04:54.208908Z",
     "start_time": "2020-10-07T23:04:51.038584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1461889, 1549840)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(X_train, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T20:24:18.155130Z",
     "start_time": "2020-10-07T20:24:18.141258Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T22:52:20.251457Z",
     "start_time": "2020-10-07T22:52:19.696022Z"
    }
   },
   "outputs": [],
   "source": [
    "train_tweet_list = X_train.apply((' ').join)\n",
    "test_tweet_list = X_test.apply((' ').join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T22:52:21.591742Z",
     "start_time": "2020-10-07T22:52:21.549842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1485                      genmarie hope fix california least\n",
       "366259           httptwitpiccom 6pa1n old pic cute miss hair\n",
       "983258              spicyguy haha ah yes way experimentation\n",
       "1435115            omgosh lt3 photo httpbitlyzwrjc julesanne\n",
       "1144081                        finished watching icarly rock\n",
       "                                 ...                        \n",
       "1158917    katzy love good car boot sale loved buying old...\n",
       "355437     jclima hear much fedora every time try im disa...\n",
       "1147344                goodnight everyone god good love much\n",
       "310681                     hate say im kind impressed window\n",
       "182695                          aint aliveim breathing death\n",
       "Name: processed_tweets, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T22:52:55.004341Z",
     "start_time": "2020-10-07T22:52:54.387191Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_train = vectorizer.fit_transform(train_tweet_list)\n",
    "tfidf_test = vectorizer.transform(test_tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T22:52:56.309317Z",
     "start_time": "2020-10-07T22:52:56.284677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 31227)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T20:24:18.503327Z",
     "start_time": "2020-10-07T20:24:18.496125Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "# rf_classifier = RandomForestClassifier(n_estimators=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T20:24:18.557568Z",
     "start_time": "2020-10-07T20:24:18.508273Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "nb_train_preds = nb_classifier.predict(tfidf_train)\n",
    "nb_test_preds = nb_classifier.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T20:24:18.647970Z",
     "start_time": "2020-10-07T20:24:18.577147Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes\n",
      "Training Accuracy: 0.9993 \t\t Testing Accuracy: 0.4968\n"
     ]
    }
   ],
   "source": [
    "nb_train_score = accuracy_score(y_train, nb_train_preds)\n",
    "nb_test_score = accuracy_score(y_test, nb_test_preds)\n",
    "print(\"Multinomial Naive Bayes\")\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(nb_train_score, nb_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T20:24:27.892658Z",
     "start_time": "2020-10-07T20:24:18.653088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with sag Solver and C=0.001\n",
      "Training Accuracy: 0.5038 \t\t Testing Accuracy: 0.4968\n",
      "\n",
      "Logistic Regression with sag Solver and C=0.01\n",
      "Training Accuracy: 0.9988 \t\t Testing Accuracy: 0.4968\n",
      "\n",
      "Logistic Regression with sag Solver and C=0.1\n",
      "Training Accuracy: 0.9989 \t\t Testing Accuracy: 0.4968\n",
      "\n",
      "Logistic Regression with sag Solver and C=1\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n",
      "Logistic Regression with sag Solver and C=10\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Allie/.conda/envs/default-env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with sag Solver and C=100\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Allie/.conda/envs/default-env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with sag Solver and C=1000\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n",
      "Logistic Regression with saga Solver and C=0.001\n",
      "Training Accuracy: 0.5036 \t\t Testing Accuracy: 0.4964\n",
      "\n",
      "Logistic Regression with saga Solver and C=0.01\n",
      "Training Accuracy: 0.9988 \t\t Testing Accuracy: 0.4968\n",
      "\n",
      "Logistic Regression with saga Solver and C=0.1\n",
      "Training Accuracy: 0.9989 \t\t Testing Accuracy: 0.4968\n",
      "\n",
      "Logistic Regression with saga Solver and C=1\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n",
      "Logistic Regression with saga Solver and C=10\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Allie/.conda/envs/default-env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with saga Solver and C=100\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Allie/.conda/envs/default-env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with saga Solver and C=1000\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n",
      "Logistic Regression with lbfgs Solver and C=0.001\n",
      "Training Accuracy: 0.5038 \t\t Testing Accuracy: 0.4968\n",
      "\n",
      "Logistic Regression with lbfgs Solver and C=0.01\n",
      "Training Accuracy: 0.9988 \t\t Testing Accuracy: 0.4968\n",
      "\n",
      "Logistic Regression with lbfgs Solver and C=0.1\n",
      "Training Accuracy: 0.9989 \t\t Testing Accuracy: 0.4968\n",
      "\n",
      "Logistic Regression with lbfgs Solver and C=1\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n",
      "Logistic Regression with lbfgs Solver and C=10\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Allie/.conda/envs/default-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with lbfgs Solver and C=100\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n",
      "Logistic Regression with lbfgs Solver and C=1000\n",
      "Training Accuracy: 0.9995 \t\t Testing Accuracy: 0.5106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Allie/.conda/envs/default-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "solvers = ['sag','saga','lbfgs']\n",
    "C = [.001, .01, .1, 1, 10, 100,1000]\n",
    "\n",
    "for solver in solvers:\n",
    "    for c in C:\n",
    "        lr_classifier = LogisticRegression(verbose=0, solver=solver,C=c, random_state=0)\n",
    "        lr_model = lr_classifier.fit(tfidf_train, y_train)\n",
    "    \n",
    "        lr_train_score = lr_model.score(tfidf_train, y_train)\n",
    "        lr_test_score = lr_model.score(tfidf_test, y_test)\n",
    "    \n",
    "        print(f\"Logistic Regression with {solver} Solver and C={c}\")\n",
    "        print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\\n\".format(lr_train_score, lr_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T20:33:44.827197Z",
     "start_time": "2020-10-07T20:31:39.413447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.3.1-cp38-cp38-macosx_10_14_x86_64.whl (165.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 165.2 MB 21 kB/s  eta 0:00:01   |█▌                              | 7.5 MB 2.5 MB/s eta 0:01:03     |███▏                            | 16.5 MB 1.5 MB/s eta 0:01:38     |███████                         | 36.1 MB 1.9 MB/s eta 0:01:09     |███████▋                        | 39.0 MB 2.8 MB/s eta 0:00:45     |██████████▏                     | 52.4 MB 2.2 MB/s eta 0:00:51███████▏                  | 67.8 MB 2.3 MB/s eta 0:00:42     |██████████████▋                 | 75.7 MB 1.2 MB/s eta 0:01:12     |███████████████                 | 77.5 MB 1.9 MB/s eta 0:00:47     |████████████████▎               | 84.3 MB 1.3 MB/s eta 0:01:04     |███████████████████▌            | 100.5 MB 2.6 MB/s eta 0:00:26     |███████████████████▉            | 102.6 MB 5.0 MB/s eta 0:00:13     |████████████████████            | 102.8 MB 5.0 MB/s eta 0:00:13     |█████████████████████           | 108.3 MB 1.7 MB/s eta 0:00:34     |██████████████████████▏         | 114.6 MB 4.7 MB/s eta 0:00:11     |████████████████████████        | 123.9 MB 1.3 MB/s eta 0:00:33     |████████████████████████▉       | 128.2 MB 1.8 MB/s eta 0:00:21     |███████████████████████████▉    | 143.6 MB 2.5 MB/s eta 0:00:09     |██████████████████████████████▊ | 158.5 MB 6.5 MB/s eta 0:00:02     |███████████████████████████████▌| 162.9 MB 162 kB/s eta 0:00:15\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.32.0-cp38-cp38-macosx_10_9_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 2.5 MB/s eta 0:00:01     |█████████▎                      | 972 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp38-cp38-macosx_10_9_x86_64.whl (15.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.1 MB 1.4 MB/s eta 0:00:01    |████▌                           | 2.1 MB 2.4 MB/s eta 0:00:06\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.13.0-cp38-cp38-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Using cached tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp38-cp38-macosx_10_9_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<3,>=2.3.0\n",
      "  Using cached tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/Allie/.conda/envs/default-env/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/Allie/.conda/envs/default-env/lib/python3.8/site-packages (from tensorflow) (0.35.1)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Using cached wrapt-1.12.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: setuptools in /Users/Allie/.conda/envs/default-env/lib/python3.8/site-packages (from protobuf>=3.9.2->tensorflow) (50.3.0.post20201006)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3-py3-none-any.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/Allie/.conda/envs/default-env/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.22.1-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/Allie/.conda/envs/default-env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Allie/.conda/envs/default-env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/Allie/.conda/envs/default-env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/Allie/.conda/envs/default-env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=2b45d924465957d70b39864183acf223c538a3e2df1796a957580d15ab356e6a\n",
      "  Stored in directory: /Users/Allie/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-macosx_10_9_x86_64.whl size=32618 sha256=7237bc5067292bcfb541504694d58851010a33664b531a9850332c4c34eedb1b\n",
      "  Stored in directory: /Users/Allie/Library/Caches/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: grpcio, numpy, protobuf, opt-einsum, gast, tensorflow-estimator, google-pasta, keras-preprocessing, h5py, markdown, tensorboard-plugin-wit, oauthlib, requests-oauthlib, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, google-auth-oauthlib, absl-py, werkzeug, tensorboard, termcolor, astunparse, wrapt, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "      Successfully uninstalled numpy-1.19.2\n",
      "Successfully installed absl-py-0.10.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.22.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.3 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0 werkzeug-1.0.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T20:33:55.747317Z",
     "start_time": "2020-10-07T20:33:47.217037Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T23:11:37.235055Z",
     "start_time": "2020-09-24T23:10:09.493008Z"
    }
   },
   "outputs": [],
   "source": [
    "dense_matrix = tfidf_train.todense()\n",
    "dense_list = dense_matrix.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:20:16.337098Z",
     "start_time": "2020-09-29T18:20:16.329554Z"
    }
   },
   "outputs": [],
   "source": [
    "# AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# tfidf_train = tfidf_train.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# tfidf_test = tfidf_test.cache().prefetch(buffer_size=AUTOTUNE)b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-default-env] *",
   "language": "python",
   "name": "conda-env-.conda-default-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
