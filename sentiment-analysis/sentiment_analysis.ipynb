{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of COVID-19 Tweets: When did the Public Panic Set In? Part 4: Supervised Classification Modeling\n",
    "\n",
    "    Notebook by Allison Kelly - allisonkelly42@gmail.com\n",
    "    \n",
    "This notebook is preceded by parts <a href=\"https://github.com/akelly66/COVID-Tweet-Sentiment/blob/master/tweet-scraping/Twitter-API-Scraping.ipynb\">1</a>, <a href=\"https://github.com/akelly66/COVID-Tweet-Sentiment/blob/master/text-processing/NLP-Text-Processing.ipynb\">2</a> and 3. Part 4 will focus on the modeling portion, but is still very much in ins infancy. Markdown cells and complete documentation to come. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T21:30:39.382179Z",
     "start_time": "2020-09-01T21:30:35.105049Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from nltk import word_tokenize\n",
    "from ast import literal_eval\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T21:31:24.185796Z",
     "start_time": "2020-09-01T21:30:39.387929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>processed_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[switchfoot, httptwitpiccom, 2y1zl, awww, that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[kenichan, dived, many, time, ball, managed, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[nationwideclass, behaving, im, mad, cant, see]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                                   processed_tweets\n",
       "0         0  [switchfoot, httptwitpiccom, 2y1zl, awww, that...\n",
       "1         0  [upset, cant, update, facebook, texting, might...\n",
       "2         0  [kenichan, dived, many, time, ball, managed, s...\n",
       "3         0             [whole, body, feel, itchy, like, fire]\n",
       "4         0    [nationwideclass, behaving, im, mad, cant, see]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets = pd.read_csv('processed_train.csv', \n",
    "                       usecols=['polarity', 'processed_tweets'],\n",
    "                       # Converting string to list\n",
    "                       converters={\"processed_tweets\": literal_eval})\n",
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T21:31:24.209235Z",
     "start_time": "2020-09-01T21:31:24.195258Z"
    }
   },
   "outputs": [],
   "source": [
    "target = train_tweets['polarity']\n",
    "data = train_tweets['processed_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T21:31:25.131018Z",
     "start_time": "2020-09-01T21:31:24.214388Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42, test_size=.20)\n",
    "\n",
    "train_df = pd.concat([X_train, y_train], axis=1) \n",
    "test_df = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T21:31:25.167237Z",
     "start_time": "2020-09-01T21:31:25.135170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    640506\n",
       "4    639494\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T21:31:56.453576Z",
     "start_time": "2020-09-01T21:31:56.340684Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sample = train_df.sample(n=100000, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T21:31:57.532048Z",
     "start_time": "2020-09-01T21:31:57.129695Z"
    }
   },
   "outputs": [],
   "source": [
    "all_words_list = [item for sublist in train_sample.processed_tweets for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T21:31:58.744476Z",
     "start_time": "2020-09-01T21:31:58.562480Z"
    }
   },
   "outputs": [],
   "source": [
    "total_vocab = set(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T21:31:59.902631Z",
     "start_time": "2020-09-01T21:31:59.892704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104827"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(all_words_list))\n",
    "len(total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T21:32:10.661864Z",
     "start_time": "2020-09-01T21:32:08.488029Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "glove = {}\n",
    "with open('glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocab:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T21:32:10.685010Z",
     "start_time": "2020-09-01T21:32:10.666438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.54289 ,  0.053743, -0.46978 ,  0.17652 , -0.33742 ,  0.16731 ,\n",
       "        1.0061  ,  0.23131 , -0.99158 ,  0.98234 , -0.33188 ,  0.64598 ,\n",
       "        0.071642, -0.28759 , -0.30952 , -0.96751 ,  0.22189 ,  0.84231 ,\n",
       "        0.55649 , -0.50558 , -0.28354 ,  0.5827  ,  1.7962  ,  0.44665 ,\n",
       "        0.046912,  0.99483 ,  0.086517,  0.75013 ,  0.11787 , -0.68936 ,\n",
       "       -0.91459 ,  0.36019 , -0.68144 ,  0.55527 , -1.0641  , -0.29523 ,\n",
       "        0.033354, -1.111   ,  0.71948 ,  0.66059 ,  0.89162 ,  0.14977 ,\n",
       "       -1.4292  ,  0.067031,  0.12727 , -0.21811 , -0.51361 ,  0.20745 ,\n",
       "       -0.074958,  0.080575], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['lol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T21:32:13.509975Z",
     "start_time": "2020-09-01T21:32:13.487644Z"
    }
   },
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # it can't be used in a scikit-learn pipeline  \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T21:32:14.043533Z",
     "start_time": "2020-09-01T21:32:14.034810Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "rf =  Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T21:32:15.548120Z",
     "start_time": "2020-09-01T21:32:15.542850Z"
    }
   },
   "outputs": [],
   "source": [
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Logistic Regression', lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-01T21:32:16.249Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed: 21.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   40.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed: 24.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   51.7s finished\n"
     ]
    }
   ],
   "source": [
    "scores = [(name, cross_val_score(model, X_train, y_train, cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-01T21:32:55.983Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Training data mean cross validation scores:/n\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T21:31:38.777057Z",
     "start_time": "2020-09-01T21:30:35.200Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_scores = [(name, accuracy(model, X_train, y_train, cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-learn-env]",
   "language": "python",
   "name": "conda-env-anaconda3-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
