{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where to get training data: http://help.sentiment140.com/for-students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:14:43.762453Z",
     "start_time": "2020-08-27T18:14:43.757065Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import tweet_proccess_funcs\n",
    "from tweet_proccess_funcs import process_tweet\n",
    "from gensim.models import word2vec\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T20:09:08.350458Z",
     "start_time": "2020-08-27T20:09:00.809548Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['polarity', 'tweet_id', 'date', 'query', 'twitter_handle', 'tweet']\n",
    "training = pd.read_csv('processed_train.csv',encoding='latin-1',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T20:09:08.874811Z",
     "start_time": "2020-08-27T20:09:08.355152Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>twitter_handle</th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1008584</td>\n",
       "      <td>4</td>\n",
       "      <td>1880812384</td>\n",
       "      <td>Fri May 22 02:35:36 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ceno_byte</td>\n",
       "      <td>@Ashtarte Ahh.   I'll sort out the router this...</td>\n",
       "      <td>['ashtarte', 'ahh', 'ill', 'sort', 'router', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1434081</td>\n",
       "      <td>4</td>\n",
       "      <td>2060563625</td>\n",
       "      <td>Sat Jun 06 19:08:29 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>leahhitchens</td>\n",
       "      <td>@davidkillimayer I would like you to know I wi...</td>\n",
       "      <td>['davidkillimayer', 'would', 'like', 'know', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538718</td>\n",
       "      <td>0</td>\n",
       "      <td>2199191924</td>\n",
       "      <td>Tue Jun 16 17:08:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>misfit_narciss</td>\n",
       "      <td>Were all just broken hearts</td>\n",
       "      <td>['broken', 'heart']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1181836</td>\n",
       "      <td>4</td>\n",
       "      <td>1982150701</td>\n",
       "      <td>Sun May 31 10:37:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>__zues</td>\n",
       "      <td>Junkyard was fun, Steve wore my glasses, http:...</td>\n",
       "      <td>['junkyard', 'fun', 'steve', 'wore', 'glass', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18612</td>\n",
       "      <td>0</td>\n",
       "      <td>1556643647</td>\n",
       "      <td>Sat Apr 18 23:41:41 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mistersaxon</td>\n",
       "      <td>@JohnKremer and 85% of statistics are made up ...</td>\n",
       "      <td>['johnkremer', 'statistic', 'made', 'spot', 'h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity    tweet_id                          date     query  \\\n",
       "1008584         4  1880812384  Fri May 22 02:35:36 PDT 2009  NO_QUERY   \n",
       "1434081         4  2060563625  Sat Jun 06 19:08:29 PDT 2009  NO_QUERY   \n",
       "538718          0  2199191924  Tue Jun 16 17:08:56 PDT 2009  NO_QUERY   \n",
       "1181836         4  1982150701  Sun May 31 10:37:46 PDT 2009  NO_QUERY   \n",
       "18612           0  1556643647  Sat Apr 18 23:41:41 PDT 2009  NO_QUERY   \n",
       "\n",
       "         twitter_handle                                              tweet  \\\n",
       "1008584       ceno_byte  @Ashtarte Ahh.   I'll sort out the router this...   \n",
       "1434081    leahhitchens  @davidkillimayer I would like you to know I wi...   \n",
       "538718   misfit_narciss                       Were all just broken hearts    \n",
       "1181836          __zues  Junkyard was fun, Steve wore my glasses, http:...   \n",
       "18612       mistersaxon  @JohnKremer and 85% of statistics are made up ...   \n",
       "\n",
       "                                          processed_tweets  \n",
       "1008584  ['ashtarte', 'ahh', 'ill', 'sort', 'router', '...  \n",
       "1434081  ['davidkillimayer', 'would', 'like', 'know', '...  \n",
       "538718                                 ['broken', 'heart']  \n",
       "1181836  ['junkyard', 'fun', 'steve', 'wore', 'glass', ...  \n",
       "18612    ['johnkremer', 'statistic', 'made', 'spot', 'h...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = training.sample(frac=.1)\n",
    "print(len(training))\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T20:12:51.324473Z",
     "start_time": "2020-08-27T20:12:51.282266Z"
    }
   },
   "outputs": [],
   "source": [
    "training = training.drop('processed_tweets',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T20:16:08.352988Z",
     "start_time": "2020-08-27T20:14:27.901400Z"
    }
   },
   "outputs": [],
   "source": [
    "training['processed_tweets'] = training.tweet.apply(process_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T18:14:54.692498Z",
     "start_time": "2020-08-27T18:14:53.928812Z"
    }
   },
   "outputs": [],
   "source": [
    "training['processed_tweets'] = training.processed_tweets.apply(lambda x: x[1:-1].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T20:16:33.387125Z",
     "start_time": "2020-08-27T20:16:33.377672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1008584    [ashtarte, ahh, ill, sort, router, weekend, like]\n",
       "1434081    [davidkillimayer, would, like, know, opining, ...\n",
       "538718                                       [broken, heart]\n",
       "1181836    [junkyard, fun, steve, wore, glass, httptinycc...\n",
       "18612      [johnkremer, statistic, made, spot, httpisgdrl...\n",
       "Name: processed_tweets, dtype: object"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training['processed_tweets'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T20:16:47.299506Z",
     "start_time": "2020-08-27T20:16:47.294303Z"
    }
   },
   "outputs": [],
   "source": [
    "target = training['polarity']\n",
    "data = training['processed_tweets'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T20:17:08.298648Z",
     "start_time": "2020-08-27T20:17:08.175172Z"
    }
   },
   "outputs": [],
   "source": [
    "all_words_list = [item for sublist in training.processed_tweets for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T20:18:36.169226Z",
     "start_time": "2020-08-27T20:18:36.022951Z"
    }
   },
   "outputs": [],
   "source": [
    "total_vocab = set(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T20:18:36.680388Z",
     "start_time": "2020-08-27T20:18:36.671718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1238957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "149926"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(all_words_list))\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T20:18:51.184292Z",
     "start_time": "2020-08-27T20:18:49.099077Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "glove = {}\n",
    "with open('glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocab:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T20:19:01.651887Z",
     "start_time": "2020-08-27T20:19:01.630544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.54289 ,  0.053743, -0.46978 ,  0.17652 , -0.33742 ,  0.16731 ,\n",
       "        1.0061  ,  0.23131 , -0.99158 ,  0.98234 , -0.33188 ,  0.64598 ,\n",
       "        0.071642, -0.28759 , -0.30952 , -0.96751 ,  0.22189 ,  0.84231 ,\n",
       "        0.55649 , -0.50558 , -0.28354 ,  0.5827  ,  1.7962  ,  0.44665 ,\n",
       "        0.046912,  0.99483 ,  0.086517,  0.75013 ,  0.11787 , -0.68936 ,\n",
       "       -0.91459 ,  0.36019 , -0.68144 ,  0.55527 , -1.0641  , -0.29523 ,\n",
       "        0.033354, -1.111   ,  0.71948 ,  0.66059 ,  0.89162 ,  0.14977 ,\n",
       "       -1.4292  ,  0.067031,  0.12727 , -0.21811 , -0.51361 ,  0.20745 ,\n",
       "       -0.074958,  0.080575], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['lol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T20:20:54.562429Z",
     "start_time": "2020-08-27T20:20:54.553772Z"
    }
   },
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # it can't be used in a scikit-learn pipeline  \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T20:21:31.901303Z",
     "start_time": "2020-08-27T20:21:31.596651Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T20:21:46.808294Z",
     "start_time": "2020-08-27T20:21:46.802815Z"
    }
   },
   "outputs": [],
   "source": [
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Logistic Regression', lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T21:26:14.574858Z",
     "start_time": "2020-08-27T20:22:23.034458Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.4s finished\n",
      "/Users/Allie/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/Allie/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T21:26:14.586669Z",
     "start_time": "2020-08-27T21:26:14.578517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.67275),\n",
       " ('Support Vector Machine', 0.69471875),\n",
       " ('Logistic Regression', 0.66435)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-learn-env]",
   "language": "python",
   "name": "conda-env-anaconda3-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
