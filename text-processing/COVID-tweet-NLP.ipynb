{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of COVID-19 Tweets: When did the Public Panic Set In?\n",
    "\n",
    "    Notebook by Allison Kelly - allisonkelly42@gmail.com\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T18:26:10.548298Z",
     "start_time": "2020-06-12T18:26:08.026565Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Generic Imports\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100) # See more text\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, time\n",
    "\n",
    "# Get JSON\n",
    "import json\n",
    "\n",
    "# Text preprocessing libraries\n",
    "import string\n",
    "import contractions\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Exploratory data analysis libraries\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Data\n",
    "\n",
    "View method to obtain data <a href=\"https://github.com/akelly66/COVID-Tweet-Sentiment/blob/master/tweet-scraping/COVID-tweets-true.ipynb\">here</a>. <br>\n",
    "<br>The tweet query parameters were as follows:\n",
    "\n",
    "- <b>Keywords: </b> \"coronavirus OR Wuhan virus OR 2019-nCoV OR China flu\"<br>\n",
    "- <b>Date Range: </b> 28 Jan 2020 - 03 Feb 2020<br>\n",
    "- <b>Location:</b> United States of America<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T18:26:11.235241Z",
     "start_time": "2020-06-12T18:26:10.553827Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"expanded_query_tweets.csv\")\n",
    "df.drop_duplicates(inplace=True)\n",
    "df = df.query(\"lang == 'en'\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T18:26:11.352496Z",
     "start_time": "2020-06-12T18:26:11.242113Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(df.info())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T18:26:11.368052Z",
     "start_time": "2020-06-12T18:26:11.355338Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "test = ast.literal_eval(df.retweeted_status[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T18:26:11.388136Z",
     "start_time": "2020-06-12T18:26:11.377390Z"
    }
   },
   "outputs": [],
   "source": [
    "test['extended_tweet']['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T18:26:11.402238Z",
     "start_time": "2020-06-12T18:26:11.393294Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_full_tweet(series):\n",
    "    series = series.dropna()\n",
    "    full_tweets = []\n",
    "    for value in series:\n",
    "   \n",
    "        converted_value = ast.literal_eval(value)\n",
    "        full_tweet = converted_value['text']\n",
    "        full_tweets.append(full_tweet)\n",
    "    \n",
    "    extended_tweet_df = pd.DataFrame(full_tweets, index=series.index, columns=['full_tweet'])\n",
    "    return extended_tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T18:26:12.842503Z",
     "start_time": "2020-06-12T18:26:11.410250Z"
    }
   },
   "outputs": [],
   "source": [
    "extended_tweets = get_full_tweet(df.retweeted_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T18:26:12.879013Z",
     "start_time": "2020-06-12T18:26:12.849853Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.join(df, extended_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T18:26:12.897359Z",
     "start_time": "2020-06-12T18:26:12.885329Z"
    }
   },
   "outputs": [],
   "source": [
    "df['full_tweet'].fillna(df['text'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing portion of this project will only include processing text data, so we'll single out that column now. Further preprocessing on the full dataset will be included in the following section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T18:26:12.955863Z",
     "start_time": "2020-06-12T18:26:12.915423Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_df = df.loc[:,['created_at','full_tweet']]\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(row):\n",
    "    '''\n",
    "    This function takes each tweet\n",
    "    and removes the urls from them\n",
    "    for easier processing.\n",
    "    '''\n",
    "    \n",
    "    row = re.sub(r'http\\S+', \"\", row)\n",
    "    return row\n",
    "\n",
    "tweet_df.full_tweet = tweet_df.full_tweet.apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T19:11:42.833077Z",
     "start_time": "2020-06-16T19:11:42.714803Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    \n",
    "    '''\n",
    "    This function takes a tweet variable,\n",
    "    removes punctuation and linebreaks,\n",
    "    sets all words to lowercase, and \n",
    "    returns the cleaned tweet as a single\n",
    "    variable list.\n",
    "    '''\n",
    "    \n",
    "    # Grabbing most common punctuation symbols and ellipsis symbol\n",
    "    punctuation_list = list(string.punctuation)+ [\"â€¦\"]\n",
    "    punctuation_list.remove('#')\n",
    "    \n",
    "    cleaned_tweet = []\n",
    "    \n",
    "    for symbol in punctuation_list:\n",
    "        tweet = tweet.replace(symbol, \"\").lower()\n",
    "        tweet = tweet.rstrip()\n",
    "      \n",
    "    cleaned_tweet.append(tweet)\n",
    "    \n",
    "    return cleaned_tweet\n",
    "\n",
    "cleaned_tweet_test = clean_tweet(tweet_df.full_tweet[3])\n",
    "cleaned_tweet_test        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T19:14:19.711855Z",
     "start_time": "2020-06-16T19:14:19.694849Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(clean_tweet):\n",
    "    \n",
    "    '''\n",
    "    This function takes a cleaned tweet,\n",
    "    joins into one string (if not already),\n",
    "    runs the tweet through NLTK work tokenizer, \n",
    "    removes English stopwords, and returns\n",
    "    the tokenized tweet in list format.\n",
    "    '''\n",
    "    \n",
    "    joined_tweet = ' '.join(clean_tweet)\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    \n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokenized_tweet = tokenizer.tokenize(joined_tweet)\n",
    "    tokenized_tweet = [word for word in tokenized_tweet if word not in stopwords_list]\n",
    "    return tokenized_tweet\n",
    "\n",
    "    \n",
    "\n",
    "tokenized_tweet_test = tokenize(cleaned_tweet_test)\n",
    "tokenized_tweet_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lem_tweet(tweet):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    lemmed_tweet = [lemmatizer.lemmatize(word) for word in tweet]\n",
    "    \n",
    "    return lemmed_tweet\n",
    "\n",
    "lemmed_tweet_test = lem_tweet(tokenized_tweet_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stem_tweet(tweet):\n",
    "    \n",
    "#     stemmer = SnowballStemmer('english')\n",
    "#     stemmed_tweet = [stemmer.stem(word) for word in tweet]\n",
    "    \n",
    "#     return stemmed_tweet\n",
    "\n",
    "# stem_test = stem_tweet(no_url_test)\n",
    "# stem_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T20:54:28.399504Z",
     "start_time": "2020-06-16T20:54:26.393822Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \n",
    "    cleaned = clean_tweet(tweet)\n",
    "    tokenized = tokenize(cleaned)\n",
    "#     stemmed_tweet = stem_tweet(tokenized)\n",
    "    lemmed_tweet = lem_tweet(tokenized)\n",
    "    \n",
    "    return lemmed_tweet\n",
    "\n",
    "tweet_df['processed_tweets'] = tweet_df['full_tweet'].apply(process_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T20:54:30.859385Z",
     "start_time": "2020-06-16T20:54:30.811864Z"
    }
   },
   "outputs": [],
   "source": [
    "tweet_df = tweet_df.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [item for sublist in tweet_df.processed_tweets for item in sublist]\n",
    "all_words = (\" \").join(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
