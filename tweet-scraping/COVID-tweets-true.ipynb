{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of COVID-19 Tweets: When did the Public Panic Set In?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Notebook by Allison Kelly - allisonkelly42@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "<i>This notebook is part one of my NLP project aiming to scrape and analyze tweets regarding the coronavirus pandemic. <br>View part two <a href=\"https://github.com/akelly66/COVID-Tweet-Sentiment/blob/master/text-processing/COVID-tweet-NLP.ipynb\">here</a>.</i>\n",
    "\n",
    "Love it or hate it, social media has gone from angsty teenagers posting poetry on LiVEJOURNAL to the leader of the free world <a href=\"https://twitter.com/realDonaldTrump/status/1213919480574812160?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1213919480574812160&ref_url=https%3A%2F%2Fmashable.com%2Farticle%2Ftrump-tweets-congress-war-powers-act%2F\">waging war on Iran</a> in 265 words (or less) in a matter of years. Social media provides analysts with <i><a href=\"https://seedscientific.com/how-much-data-is-created-every-day/\">zettabytes</a></i> of data every day. That's 1,000 bytes to the seventh power. Specifically, Twitter users generate 500 million tweets per day, of which the content of those Tweets contains invaluable public opinion data. \n",
    "\n",
    "As the world has been turned on its head during the COVID-19 global pandemic, an interesting question arises. How seriously is the public taking the pandemic? Millions have lost their jobs, deaths from the disease are in the hundreds each day, and the US meat supply chain is on the brink of failure, but life-saving shelter-in-place orders are being defied as protesters rally all over the country in favor of opening the ecomony. \n",
    "\n",
    "The question I seek to answer in this project is whether the public opinion of how serious the pandemic was changed in the United States once the World Health Organization declared a global pandemic. This is not meant to be academically rigorous. As I only have access to a Premium API Sandbox dev environment, I had to scale down my query to only include tweets containing two keywords, \"COVID-19\" and \"coronavirus,\" and a limit of 5,000 tweets scraped per month. I chose the 24 hours before and after the declaration on January 31, 2020, though this will give a limited scope as stay-at-home orders did not begin until mid-March. \n",
    "\n",
    "The following code will demonstrate how I scraped tweets using the <a href=\"https://github.com/twitterdev/search-tweets-python\">searchtweets</a> and <a href=\"https://pypi.org/project/requests/2.7.0/\">requests</a> packages once connected to the Twitter API. \n",
    "\n",
    "<b> More documentation to come.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:44:49.473741Z",
     "start_time": "2020-05-18T17:44:48.177614Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import searchtweets\n",
    "from searchtweets import load_credentials\n",
    "from string import Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OAUTH 2.0 Bearer Token\n",
    "\n",
    "After following the Twitter Developer <a href=\"https://developer.twitter.com/en/docs/basics/getting-started\">Getting Started</a> guide, I created a developer app, received API keys, and generated a bearer token from OAuth 2.0. My API keys are housed in a secret YAML document, separate from the repository used to house this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T19:56:38.291965Z",
     "start_time": "2020-05-18T19:56:38.014221Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Allie/anaconda3/lib/python3.7/site-packages/searchtweets/credentials.py:34: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  search_creds = yaml.load(f)[yaml_key]\n",
      "Grabbing bearer token from OAUTH\n"
     ]
    }
   ],
   "source": [
    "credentials = load_credentials(filename=\"/Users/Allie/Documents/DS-Projects/API keys/twitter_keys.yaml\",\n",
    "                 yaml_key=\"search_tweets_api\",\n",
    "                 env_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T19:56:42.448287Z",
     "start_time": "2020-05-18T19:56:42.442306Z"
    }
   },
   "outputs": [],
   "source": [
    "bearer_token = credentials['bearer_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Request\n",
    "\n",
    "In order to test my search parameters, I'm making an initial request to the Twitter API Full Archive. The parameters I'm using for this project are the following:\n",
    "\n",
    "   - <b>Keywords: </b> \"coronavirus\" and \"COVID-19\"<br>\n",
    "   <br> The number of tweets would be too large for the access I have, and though I would love to include many variations such as \"pandemic,\" \"Wuhan virus,\" or other related terms, I chose the two most common that were COVID-19 specific. The WHO did not release the name COVID-19 until February 11, 2020 so once my requests reset, I will not be able to change this. <br><br>\n",
    "   - <b>Date Range: </b> 12:00:00 30 Jan 2020 - 23:59:00  31 Jan 2020<br><br>\n",
    "   The Twitter API dates parameter is exlusive of the last minute. Again, once my requests are reset, I'll change this to reflect tweets the day before, the day of, and the day after the WHO declared a pandemic on 30 Jan 2020.\n",
    "   <br><br>\n",
    "   - <b>Location:</b> United States of America<br><br>\n",
    "   By using the profile_country parameter, the selected tweets will be tweets or retweets of profiles that indicate the US is their geographical location, though they may have tweeted from abroad or retweeted non-Americans. I don't believe this should alter the intended sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T17:54:49.149408Z",
     "start_time": "2020-05-18T17:54:48.883693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Full archive endpoint\n",
    "endpoint = 'https://api.twitter.com/1.1/tweets/search/fullarchive/production.json'\n",
    "\n",
    "headers = {\"Authorization\":\"Bearer {}\".format(bearer_token), \n",
    "           \"Content-Type\": \"application/json\"}  \n",
    "\n",
    "data = '{\"query\":\"(coronavirus OR COVID-19)\", \n",
    "        \"fromDate\": \"202001300000\", \n",
    "        \"toDate\": \"202001312359\"}, \n",
    "        {\"place\":{\"profile_country\":\"United States of America\"}}'\n",
    "\n",
    "# POST request instead of GET to avoid URL length restrictions of 2048 characters\n",
    "response = requests.post(endpoint,data=data,headers=headers).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use JSON to decode the response into a \"pretty-printed\" string to explore the results of the request to see if everything checks out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T22:56:15.984122Z",
     "start_time": "2020-05-19T22:56:15.953234Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"error\": {\n",
      "    \"message\": \"Request exceeds account\\u2019s current package request limits. Please upgrade your package and retry or contact Twitter about enterprise access.\",\n",
      "    \"sent\": \"2020-05-18T17:54:49+00:00\",\n",
      "    \"transactionId\": \"001d6c5e00207d72\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Indenting to pretty-print\n",
    "print(json.dumps(response, indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first tweet from the request shows the first tweet in this time period containing the keyword \"coronavirus\" wasn't unil Fri Jan 31, 2020 at 23:55:39. I find it hard to believe no one tweeted anything with \"coronavirus\" in the US before this time. I'll do more investigation once my request limits reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T18:33:51.241674Z",
     "start_time": "2020-05-02T18:33:51.230898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating pandas dataframe from result\n",
    "\n",
    "covid_tweets = pd.DataFrame.from_dict(response['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pagination\n",
    "\n",
    "Rate limits for the free Sandbox dev include 100 tweets per request and 30 requests per minute. The response includes a \"next token\" that can be supplied to subsequent queries in order to paginate through the results until there are no longer tweets that fit your query. \n",
    "\n",
    "I've created a helper function to pull the unique next token per request and paginate through the results. Each JSON string will then be added to the dataframe defined by the initial API request. This function can be generalized by substituting the data and template variables with a properly formatted JSON query.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T12:54:28.028468Z",
     "start_time": "2020-05-10T12:54:28.010468Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 tweets per request\n",
    "len(covid_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_next_token(endpoint, headers, query=data):\n",
    "    response = requests.post(endpoint,data=data,headers=headers).json()\n",
    "    \n",
    "    if response['next']:\n",
    "        return response['next']\n",
    "    else:\n",
    "        return 'Error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T18:26:07.319788Z",
     "start_time": "2020-05-21T18:26:07.308604Z"
    }
   },
   "outputs": [],
   "source": [
    "def paginate_covid_tweets(endpoint, headers, next_token):\n",
    "    \n",
    "    \"\"\"\n",
    "    This helper function when combined with a loop will pull the next_token \n",
    "    from each request and paginate through Twitter API requests. \n",
    "    \n",
    "    Keyword arguments:\n",
    "    \n",
    "    endpoint -- URL string of Twitter API endpoint including dev environment name\n",
    "    headers -- Dictonary of OAuth bearer token and content type\n",
    "    next_token -- Next_token from previous \n",
    "    request from which to begin pagination\n",
    "    \"\"\" \n",
    "    \n",
    "    t=Template('{\"query\":\"(coronavirus OR COVID-19)\", \"fromDate\": \"202001300000\", \"toDate\": \"202001312359\", \"next\":\"${next_token}\"}, {\"place\":{\"profile_country\":\"United States of America\"}}')\n",
    "    data = t.substitute(next_token=next_token)\n",
    "    response = requests.post(endpoint, data=data, headers=headers).json()\n",
    "    \n",
    "        \n",
    "    if response['next']:\n",
    "               \n",
    "        next_token = response['next']\n",
    "        return response, next_token\n",
    "    \n",
    "    else:\n",
    "        return response, \"End of Pagination\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def paginate_covid_tweets(endpoint, headers, previous_response_token=None):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     This helper function when combined with a loop with pull the next_token \n",
    "#     from each request and paginate through Twitter API requests. \n",
    "    \n",
    "#     Keyword arguments:\n",
    "    \n",
    "#     endpoint -- URL string of Twitter API endpoint including dev environment name\n",
    "#     headers -- Dictonary of OAuth bearer token and content type\n",
    "#     previous_response_token -- None or next_token from previous \n",
    "#     request from which to begin pagination\n",
    "#     \"\"\" \n",
    "    \n",
    "#     global next_token\n",
    "    \n",
    "#     if previous_response_token:\n",
    "#         t=Template('{\"query\":\"(coronavirus OR COVID-19)\",\n",
    "#                    \"fromDate\": \"202001300000\", \n",
    "#                    \"toDate\": \"202001312359\",\n",
    "#                    \"next\":\"${next_token}\"}, \n",
    "#                    {\"place\":{\"profile_country\":\"United States of America\"}}')\n",
    "#         data = t.substitute(next_token=previous_response_token)\n",
    "#         response = requests.post(endpoint, data=data, headers=headers).json()\n",
    "#         next_token = response['next']\n",
    "        \n",
    "#         return response, next_token\n",
    "    \n",
    "#     elif previous_response_token == False: \n",
    "        \n",
    "#         data = '{\"query\":\"(coronavirus OR COVID-19)\", \n",
    "#                    \"fromDate\": \"202001300000\", \n",
    "#                    \"toDate\": \"202001312359\",\n",
    "#                    \"next\":\"${next_token}\"}, \n",
    "#                    {\"place\":{\"profile_country\":\"United States of America\"}}'\n",
    "#         response = requests.post(endpoint, data=data, headers=headers).json()\n",
    "#         next_token = response['next']\n",
    "        \n",
    "#         return response, next_token\n",
    "    \n",
    "#     else:\n",
    "#         return \"No next token.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the helper function, we'll identify the next token from the initial request, then loop through each request with the pagination function, assigning a new token to each request, and add each page to a list until we've hit the monthly rate limit of 50 requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning next token to variable\n",
    "next_token = get_initial_next_token(endpoint, headers, query=data)\n",
    "\n",
    "# Starting iteration at 0\n",
    "i = 0\n",
    "dfs = []\n",
    "\n",
    "# Loop until rate limit of 50 is met\n",
    "while i < 51:\n",
    "    response, next_token = paginate_covid_tweets(endpoint, \n",
    "                                                 headers, \n",
    "                                                 next_token)\n",
    "    dfs.append(response)\n",
    "    i += 1\n",
    "    \n",
    "else:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll take just the tweet results, convert to dataframes, and add them to our initial covid_tweets dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dictionary in dfs:\n",
    "    df = pd.DataFrame.from_dict(dictionary['results'])\n",
    "    covid_tweets = pd.concat([covid_tweets,df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T21:19:47.285043Z",
     "start_time": "2020-05-14T21:19:47.276209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eyJtYXhJZCI6MTIyMzM5NDQ0MzQwMDc2NTQ0MX0='"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final token\n",
    "latest_token = dfs[-1]['next']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After my trial an error, I've ended up with 3,700 tweets. This will become more robust, again, once the limit is reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T21:21:49.440122Z",
     "start_time": "2020-05-14T21:21:49.430894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3700"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(covid_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
