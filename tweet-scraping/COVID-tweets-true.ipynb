{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of COVID-19 Tweets: When did the Public Panic Set In?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Notebook by Allison Kelly - allisonkelly42@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "<i>This notebook is part one of my NLP project aiming to scrape and analyze tweets regarding the coronavirus pandemic. <br>View part two <a href=\"https://github.com/akelly66/COVID-Tweet-Sentiment/blob/master/text-processing/COVID-tweet-NLP.ipynb\">here</a>.</i>\n",
    "\n",
    "Love it or hate it, social media has gone from angsty teenagers posting poetry on LiVEJOURNAL to the leader of the free world <a href=\"https://twitter.com/realDonaldTrump/status/1213919480574812160?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1213919480574812160&ref_url=https%3A%2F%2Fmashable.com%2Farticle%2Ftrump-tweets-congress-war-powers-act%2F\">waging war on Iran</a> in 265 words (or less) in a matter of years. Social media provides analysts with <i><a href=\"https://seedscientific.com/how-much-data-is-created-every-day/\">zettabytes</a></i> of data every day. That's 1,000 bytes to the seventh power. Specifically, Twitter users generate 500 million tweets per day, of which the content of those Tweets contains invaluable public opinion data. \n",
    "\n",
    "As the world has been turned on its head during the COVID-19 global pandemic, an interesting question arises. How seriously is the public taking the pandemic? Millions have lost their jobs, deaths from the disease are in the hundreds each day, and the US meat supply chain is on the brink of failure, but life-saving shelter-in-place orders are being defied as protesters rally all over the country in favor of opening the ecomony. \n",
    "\n",
    "The question I seek to answer in this project is whether the public opinion of how serious the pandemic was changed in the United States once the World Health Organization declared a global pandemic. This is not meant to be academically rigorous. As I only have access to a Premium API Sandbox dev environment, I had to scale down my query to only include tweets containing two keywords, \"COVID-19\" and \"coronavirus,\" and a limit of 5,000 tweets scraped per month. I chose the 24 hours before and after the declaration on January 31, 2020, though this will give a limited scope as stay-at-home orders did not begin until mid-March. \n",
    "\n",
    "The following code will demonstrate how I scraped tweets using the <a href=\"https://github.com/twitterdev/search-tweets-python\">searchtweets</a> and <a href=\"https://pypi.org/project/requests/2.7.0/\">requests</a> packages once connected to the Twitter API. \n",
    "\n",
    "<b> More documentation to come.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:53:25.164247Z",
     "start_time": "2020-06-03T15:53:25.159168Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import searchtweets\n",
    "from searchtweets import load_credentials\n",
    "from string import Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OAUTH 2.0 Bearer Token\n",
    "\n",
    "After following the Twitter Developer <a href=\"https://developer.twitter.com/en/docs/basics/getting-started\">Getting Started</a> guide, I created a developer app, received API keys, and generated a bearer token from OAuth 2.0. My API keys are housed in a secret YAML document, separate from the repository used to house this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T19:56:38.291965Z",
     "start_time": "2020-05-18T19:56:38.014221Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Allie/anaconda3/lib/python3.7/site-packages/searchtweets/credentials.py:34: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  search_creds = yaml.load(f)[yaml_key]\n",
      "Grabbing bearer token from OAUTH\n"
     ]
    }
   ],
   "source": [
    "credentials = load_credentials(filename=\"/Users/Allie/Documents/DS-Projects/API keys/twitter_keys.yaml\",\n",
    "                 yaml_key=\"search_tweets_api\",\n",
    "                 env_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T19:56:42.448287Z",
     "start_time": "2020-05-18T19:56:42.442306Z"
    }
   },
   "outputs": [],
   "source": [
    "bearer_token = credentials['bearer_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Request\n",
    "\n",
    "In order to test my search parameters, I'm making an initial request to the Twitter API Full Archive. The parameters I'm using for this project are the following:\n",
    "\n",
    "   - <b>Keywords: </b> \"coronavirus OR Wuhan virus OR 2019-nCoV OR China flu\"<br>\n",
    "   <br> This keyword list will not be exhaustive of all tweets relating to COVID-19 during this time period but I believe will be enough to get an impression of the public response to the WHO's declaration of a pandemic. <br><br>\n",
    "   - <b>Date Range: </b> 28 Jan 2020 -  03 Feb 2020<br><br>\n",
    "   The pandemic was declared on January 31, 2020. The few days that bookend this date can offer a more concrete examination of public perception pre- and post-declaration\n",
    "   <br><br>\n",
    "   - <b>Location:</b> United States of America<br><br>\n",
    "   By using the profile_country parameter, the selected tweets will be tweets or retweets of profiles that indicate the US is their geographical location, though they may have tweeted from abroad or retweeted non-Americans. I don't believe this should alter the intended sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T21:48:22.583822Z",
     "start_time": "2020-06-03T21:48:22.371630Z"
    }
   },
   "outputs": [],
   "source": [
    "# Full archive endpoint\n",
    "endpoint = 'https://api.twitter.com/1.1/tweets/search/fullarchive/production.json'\n",
    "\n",
    "headers = {\"Authorization\":\"Bearer {}\".format(bearer_token), \n",
    "           \"Content-Type\": \"application/json\"}  \n",
    "\n",
    "data = '{\"query\":\"(coronavirus OR Wuhan virus OR 2019-nCoV OR China flu) -filter:retweets AND -filter:replies AND lang:en\", \"fromDate\": \"202001280000\", \"toDate\":\"202002030000\"}, {\"place\":{\"profile_country\":\"United States of America\"}}'\n",
    "\n",
    "# POST request instead of GET to avoid URL length restrictions of 2048 characters\n",
    "response = requests.post(endpoint,data=data,headers=headers).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use JSON to decode the response into a \"pretty-printed\" string to explore the results of the request to see if everything checks out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T21:48:23.695507Z",
     "start_time": "2020-06-03T21:48:23.689526Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"error\": {\n",
      "    \"message\": \"Request exceeds account\\u2019s current package request limits. Please upgrade your package and retry or contact Twitter about enterprise access.\",\n",
      "    \"sent\": \"2020-06-03T21:48:22+00:00\",\n",
      "    \"transactionId\": \"00d3b8dc0063c899\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Indenting to pretty-print\n",
    "print(json.dumps(response, indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first tweet is in Spanish though I meant to only include those in English, not to suppress non-American or non-English speaking voices, but because this project was not meant to be mutli-lingual. There are multilingual word embeddings by Facebook and Google that I could use once my initial exploration in English is complete. It would be facinating to look into different cultural responses to the pandemic.\n",
    "\n",
    "The first tweet also includes an image that is instrumental to the understanding of the tweet. Taking a harder look, it seems as if this particular user draws comparison between the fictional Umbrella Corporation logo from the Resident Evil videogames and a defunct Shanghai biotech firm logo (Read an article about it <a href=\"https://www.snopes.com/fact-check/resident-evil-umbrella-coronavirus/\">here</a>.) While conspiratorial and silly, it does bring up a limitation of this project: there will be no sentiment analysis of images or videos, only text. I'll need to identify and eliminate any tweet that has a video or image significant to the context of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:50:58.118053Z",
     "start_time": "2020-06-03T15:50:58.099023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating pandas dataframe from result\n",
    "covid_tweets = pd.DataFrame.from_dict(response['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pagination\n",
    "\n",
    "Rate limits for the free Sandbox dev include 100 tweets per request and 30 requests per minute. The response includes a \"next token\" that can be supplied to subsequent queries in order to paginate through the results until there are no longer tweets that fit your query. \n",
    "\n",
    "I've created a helper function to pull the unique next token per request and paginate through the results. Each JSON string will then be added to the dataframe defined by the initial API request. This function can be generalized by substituting the data and template variables with a properly formatted JSON query.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:51:07.469794Z",
     "start_time": "2020-06-03T15:51:07.462790Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 tweets per request\n",
    "len(covid_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:51:11.764336Z",
     "start_time": "2020-06-03T15:51:11.758428Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_initial_next_token(endpoint, headers, query=data):\n",
    "    response = requests.post(endpoint,data=data,headers=headers).json()\n",
    "    \n",
    "    if response['next']:\n",
    "        return response['next']\n",
    "    else:\n",
    "        return 'Error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:53:33.900054Z",
     "start_time": "2020-06-03T15:53:33.892948Z"
    }
   },
   "outputs": [],
   "source": [
    "def paginate_covid_tweets(endpoint, headers, next_token):\n",
    "    \n",
    "    \"\"\"\n",
    "    This helper function when combined with a loop will pull the next_token \n",
    "    from each request and paginate through Twitter API requests. \n",
    "    \n",
    "    Keyword arguments:\n",
    "    \n",
    "    endpoint -- URL string of Twitter API endpoint including dev environment name\n",
    "    headers -- Dictonary of OAuth bearer token and content type\n",
    "    next_token -- Next_token from previous \n",
    "    request from which to begin pagination\n",
    "    \"\"\" \n",
    "    \n",
    "    t=Template('{\"query\":\"(coronavirus OR Wuhan virus OR 2019-nCoV OR China flu) -filter:retweets AND -filter:replies AND lang:en\", \"fromDate\": \"202001300000\", \"toDate\": \"202001312359\", \"next\":\"${next_token}\"}, {\"place\":{\"profile_country\":\"United States of America\"}}')\n",
    "    data = t.substitute(next_token=next_token)\n",
    "    response = requests.post(endpoint, data=data, headers=headers).json()\n",
    "    \n",
    "        \n",
    "    if response['next']:\n",
    "               \n",
    "        next_token = response['next']\n",
    "        return response, next_token\n",
    "    \n",
    "    else:\n",
    "        return response, \"End of Pagination\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the helper function, we'll identify the next token from the initial request, then loop through each request with the pagination function, assigning a new token to each request, and add each page to a list until we've hit the monthly rate limit of 50 requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:58:03.207152Z",
     "start_time": "2020-06-03T15:57:54.437949Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assigning next token to variable\n",
    "next_token = get_initial_next_token(endpoint, headers, query=data)\n",
    "\n",
    "# Starting iteration at 0\n",
    "i = 0\n",
    "dfs = []\n",
    "\n",
    "# Loop until rate limit of 50 is met\n",
    "while i < 51:\n",
    "    response, next_token = paginate_covid_tweets(endpoint, \n",
    "                                                 headers, \n",
    "                                                 next_token)\n",
    "    dfs.append(response)\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll take just the tweet results, convert to dataframes, and add them to our initial covid_tweets dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:58:09.149301Z",
     "start_time": "2020-06-03T15:58:08.755230Z"
    }
   },
   "outputs": [],
   "source": [
    "for dictionary in dfs:\n",
    "    df = pd.DataFrame.from_dict(dictionary['results'])\n",
    "    covid_tweets = pd.concat([covid_tweets,df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:58:09.997142Z",
     "start_time": "2020-06-03T15:58:09.990061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4400"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(covid_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:58:16.630535Z",
     "start_time": "2020-06-03T15:58:16.626031Z"
    }
   },
   "outputs": [],
   "source": [
    "# Final token\n",
    "latest_token = dfs[-1]['next']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After my trial and error, I've ended up with 4,400 tweets. This will become more robust, again, once the limit is reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:58:51.001183Z",
     "start_time": "2020-06-03T15:58:50.994561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4400"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(covid_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T15:59:53.774623Z",
     "start_time": "2020-06-03T15:59:52.630719Z"
    }
   },
   "outputs": [],
   "source": [
    "covid_tweets.to_csv('expanded_query_tweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There was an issue with the next token on my first run through with the pagination function, but as my requests for this month have reached their limit, I won't be able to investigate until July. The function did run again when the next token was initiated with the final token of the first run.\n",
    "<br><br>\n",
    "2. There needs to be a bit of cleanup with loops and functions.\n",
    "<br><br>\n",
    "3. Pagination needs to exhaust all tweets pertaining to this project.\n",
    "<br><br>\n",
    "4. Added filter for retweets, replies, and language on June 3. Will need to wait until limits reset to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
